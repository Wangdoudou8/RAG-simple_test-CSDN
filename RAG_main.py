# è‡ªåˆ¶æ³•å¾‹é—®ç­” RAG ğŸ¤–
import jieba  # å¼•å…¥jiebaåº“ï¼Œç”¨äºä¸­æ–‡åˆ†è¯
import pandas as pd  # å¼•å…¥pandasåº“ï¼Œç”¨äºæ•°æ®å¤„ç†
from sklearn.feature_extraction.text import TfidfVectorizer  # å¼•å…¥TfidfVectorizerï¼Œç”¨äºæ–‡æœ¬ç‰¹å¾æå–
import openai  # å¼•å…¥openaiåº“ï¼Œç”¨äºè°ƒç”¨GPTæ¨¡å‹


########################  æ­¥éª¤ 1(start)  ########################
# è¯»å–æ³•å¾‹é—®ç­”æ•°æ®
laws_content = pd.read_csv('./data/æ³•å¾‹é—®ç­”æ•°æ®.csv')
# è¿‡æ»¤æ‰å›å¤ä¸ºç©ºçš„è¡Œ
laws_content = laws_content[~laws_content['reply'].isnull()]
new_laws_content = []
print("æ³•å¾‹å†…å®¹çš„æ¡æ•°ï¼š", len(laws_content))
for i in range(len(laws_content)):
    # åˆ¤æ–­ laws_content['question'][i] çš„å†…å®¹æ˜¯ä¸æ˜¯æ–‡æœ¬
    if isinstance(laws_content['question'].iloc[i], str):
        new_laws_content.append(laws_content['title'].iloc[i] + laws_content['question'].iloc[i] + laws_content['reply'].iloc[i])
    else:
        new_laws_content.append(laws_content['title'].iloc[i] + laws_content['reply'].iloc[i])

# è¿‡æ»¤æ‰é•¿åº¦å°äº10çš„å›å¤(è¿™ä¸€æ­¥æ˜¯æ•°æ®é¢„å¤„ç†)
new_laws_content = [x for x in new_laws_content if len(x) > 10]
# å–å‰ä¸‰æ¡æ•°æ®ä½œä¸ºç¤ºä¾‹
examples = new_laws_content[:3]
print("å‰ä¸‰æ¡æ³•å¾‹å†…å®¹: ", examples)
########################  æ­¥éª¤ 1(end)  ########################

########################  æ­¥éª¤ 2(start)  ########################
# è¯»å–åœç”¨è¯
stop_words = open("./data/stopwords.txt", "r", encoding='utf-8').readlines()
# å»é™¤æ¯è¡Œæœ«å°¾çš„æ¢è¡Œç¬¦
stop_words = [x.strip() for x in stop_words]
print("åœç”¨è¯æ•°é‡: ", len(stop_words))  # æ‰“å°åœç”¨è¯æ•°é‡

# åˆå§‹åŒ–TfidfVectorizer
vectorizer = TfidfVectorizer(
    tokenizer=jieba.lcut,  # ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
    stop_words=stop_words  # åœç”¨è¯åˆ—è¡¨
)
# è·å–ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
user_question = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
# å°†æ‰€æœ‰æ³•å¾‹å†…å®¹ç”¨æ¥è®­ç»ƒæ¨¡å‹å¹¶æå–TF-IDFç‰¹å¾
laws_tfidf = vectorizer.fit_transform(new_laws_content)
# å¯¹ç”¨æˆ·é—®é¢˜è¿›è¡ŒTF-IDFç‰¹å¾æå–
user_tfidf = vectorizer.transform([user_question])

# æ‰“å°æ³•å¾‹å†…å®¹åˆ†è§£å‡ºçš„æ‰€æœ‰ç‰¹å¾åä¸­çš„æœ€å10ä¸ªå…ƒç´ 
print(vectorizer.get_feature_names_out()[10:])
# æ‰“å°TF-IDFçŸ©é˜µçš„å½¢çŠ¶
print("åœç”¨è¯æ•°é‡:", laws_tfidf.shape)
# æœ‰ 31482 ä¸ªæ–‡æ¡£ï¼Œæ¯ä¸ªæ–‡æ¡£è¢«è¡¨ç¤ºä¸ºä¸€ä¸ª 41022 ç»´çš„å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªç‹¬ç‰¹è¯æ±‡çš„TF-IDFå€¼

# è®¡ç®—ç”¨æˆ·é—®é¢˜ä¸æ³•å¾‹å†…å®¹çš„ç›¸ä¼¼åº¦
laws_similarity = user_tfidf.dot(laws_tfidf.T).toarray()[0]
# è·å–ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ä¸‰ä¸ªæ³•å¾‹å†…å®¹çš„ç´¢å¼•
top_laws = [new_laws_content[x] for x in laws_similarity.argsort()[-3:]]

# æ‰“å°æœ€ç›¸å…³çš„ä¸‰ä¸ªæ³•å¾‹å†…å®¹
print('æœ€ç›¸å…³çš„æ³•å¾‹å†…å®¹æ˜¯: ')
for idx, new in enumerate(top_laws):
    print(f'{idx + 1}. {new}')
########################  æ­¥éª¤ 2(end)  ########################

########################  æ­¥éª¤ 3(start)  ##########
# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = openai.OpenAI(
    api_key='sk-ZSSju0J2CpD1CgmNTAPPRH67i47DhgapCbQkaP86VYmge0FO',
    base_url="https://api.openai-proxy.org/v1",
)

# å‡†å¤‡å‘é€ç»™GPTçš„å†…å®¹
prompt = f"""è¯·ç»“åˆç›¸å…³çš„èµ„æ–™å›ç­”é—®é¢˜ï¼š  
æˆ‘çš„æé—®{user_question}  

è¿™æ˜¯ç›¸å…³çš„èµ„æ–™ï¼š  
{top_laws[0]}  
{top_laws[1]}  
{top_laws[2]}  
"""

# è°ƒç”¨GPTæ¨¡å‹è¿›è¡Œå›ç­”
response = client.chat.completions.create(
    model='gpt-4o-mini',  # æŒ‡å®šGPTæ¨¡å‹
    max_tokens=1000,  # è®¾ç½®æœ€å¤§tokenæ•°ï¼Œä»¥é™åˆ¶å›å¤é•¿åº¦
    # stream=True,  # è®¾ç½®ä¸ºTrueä»¥ä¿æŒå¯¹è¯è¿›è¡Œ
    messages=[{"role": "user", "content": prompt}]  # å‘é€çš„æ¶ˆæ¯
)

# è·å–GPTçš„å›å¤
res = response.choices[0].message.content
# æ‰“å°RAGé—®ç­”æœºå™¨äººçš„å›å¤
print('\nRAGé—®ç­”æœºå™¨äººçš„å›å¤ï¼š\n' + res)
########################  æ­¥éª¤ 3(end)  ########################


# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = openai.OpenAI(
    api_key='API key',  # æ›¿æ¢ä¸ºä½ çš„API key
    base_url="ä»£ç†æœåŠ¡å™¨çš„ URL",  # æ›¿æ¢ä¸ºä½ çš„ä»£ç†æœåŠ¡å™¨çš„ URL
)